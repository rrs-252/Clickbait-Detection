name: Run ML Programs with GPU
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  run-programs:
    # Using self-hosted runner with GPU
    runs-on: self-hosted
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Check GPU Status
      run: |
        nvidia-smi
        if [ $? -ne 0 ]; then
          echo "GPU not available!"
          exit 1
        fi

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install CUDA dependencies
      run: |
        # Verify CUDA installation
        nvcc --version
        # Install CUDA toolkit if needed
        # sudo apt-get install -y nvidia-cuda-toolkit
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Install PyTorch with CUDA support
        pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
        # Install other dependencies
        pip install bs4 requests nltk gensim transformers scikit-learn numpy pandas
        
    - name: Verify GPU Setup
      run: |
        python -c "import torch; print('GPU available:', torch.cuda.is_available()); print('GPU device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
    
    - name: Download NLTK data
      run: |
        python -m nltk.downloader punkt
        python -m nltk.downloader all
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache HuggingFace models
      uses: actions/cache@v3
      with:
        path: ~/.cache/huggingface
        key: ${{ runner.os }}-huggingface-${{ hashFiles('**/*.py') }}
        restore-keys: |
          ${{ runner.os }}-huggingface-
    
    - name: Run HTML Parser Preprocessor
      run: python html_parser_preprocessor.py
    
    - name: Run LDA-Bert
      run: |
        # Set environment variables for GPU memory management
        export CUDA_VISIBLE_DEVICES=0
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
        python LDA_Bert.py
    
    - name: Run Clickbait Classifier
      run: |
        export CUDA_VISIBLE_DEVICES=0
        python clickbait_classifier.py
    
    - name: GPU Memory Cleanup
      if: always()
      run: |
        nvidia-smi
        # Optional: Clear GPU memory if needed
        python -c "import torch; torch.cuda.empty_cache()"
